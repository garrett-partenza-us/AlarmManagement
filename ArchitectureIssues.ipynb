{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "sublime-kinase",
   "metadata": {},
   "source": [
    "## Addressing Our Architecture Issues\n",
    "1. **Our sigmoid function squishes predictions between 0 and 1, but our labels are between 2 and -2. How can we fix this?**\n",
    "\n",
    "Normalize the alarm labels according to our activation function in the prediction layer. For example normalize labels between -1 and 1 if using tanh, or normalize labels between 0 and 1 if using sigmoid.\n",
    "\n",
    "2. **If we decide to not use multilabel classification and instead train individual networks, is there a library that makes this easier for us?**\n",
    "\n",
    "Yes, the MultiOutputClassifier class of scikit learn is compatible with Keras classifiers\n",
    "\n",
    "3. **What is a better accuracy metric to measure model preformance considering data is sparse?**\n",
    "\n",
    "We can create custom Keras metrics functions to calculate the accuracy for the model in terms of predicting only alarm states that are on\n",
    "\n",
    "4. **What loss function should we use now that out labels are not 0 and 1?**\n",
    "\n",
    "Read notebook for my ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "ranking-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "touched-schedule",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.406\n",
      "Loss: 0.358\n",
      "Loss: 0.319\n",
      "Loss: 0.287\n",
      "Loss: 0.260\n",
      "Loss: 0.237\n",
      "Loss: 0.218\n",
      "Loss: 0.202\n",
      "Loss: 0.187\n",
      "Loss: 0.175\n",
      "Loss: 0.164\n",
      "Loss: 0.154\n",
      "Loss: 0.145\n",
      "Loss: 0.137\n",
      "Loss: 0.130\n",
      "Loss: 0.124\n",
      "Loss: 0.118\n",
      "Loss: 0.113\n",
      "Loss: 0.108\n",
      "Loss: 0.103\n"
     ]
    }
   ],
   "source": [
    "# Option 1\n",
    "# 81 alarms each with 5 possible states\n",
    "# multi-hot encoding of length 405\n",
    "# Train model using binary cross entropy loss on those labels\n",
    "\n",
    "model = nn.Linear(20, 5) \n",
    "x = torch.randn(1, 20)\n",
    "y = torch.tensor([[0,1,0,1,0]]).float()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "for epoch in range(20):\n",
    "    optimizer.zero_grad()\n",
    "    output = (model(x))\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('Loss: {:.3f}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "champion-likelihood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original labels:  [1 2 3 4 5]\n",
      "normalized labels:  [0.   0.25 0.5  0.75 1.  ]\n",
      "Loss: 0.921\n",
      "Loss: 0.841\n",
      "Loss: 0.775\n",
      "Loss: 0.719\n",
      "Loss: 0.672\n",
      "Loss: 0.633\n",
      "Loss: 0.601\n",
      "Loss: 0.574\n",
      "Loss: 0.551\n",
      "Loss: 0.532\n",
      "Loss: 0.515\n",
      "Loss: 0.501\n",
      "Loss: 0.489\n",
      "Loss: 0.479\n",
      "Loss: 0.470\n",
      "Loss: 0.462\n",
      "Loss: 0.455\n",
      "Loss: 0.449\n",
      "Loss: 0.444\n",
      "Loss: 0.439\n"
     ]
    }
   ],
   "source": [
    "# Option 2\n",
    "# Normalize alarms to be bewteen 0 and 1\n",
    "# (-2, -1, 0, 1, 2) --> (0, 1, 2, 3, 4) -- > (0., 0.25, 0.50, 0.75, 1.0)\n",
    "# Train model using binary cross entropy loss on those labels\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "labels = np.array([1,2,3,4,5])\n",
    "scaler = MinMaxScaler()\n",
    "normed_labels = scaler.fit_transform(labels.reshape(-1, 1))\n",
    "\n",
    "print(\"original labels: \", labels)\n",
    "print(\"normalized labels: \", normed_labels.reshape(-1))\n",
    "\n",
    "model = nn.Linear(20, 5) \n",
    "x = torch.randn(1, 20)\n",
    "y = torch.tensor([[0., 0.25, 0.50, 0.75, 1.0]]).float()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "for epoch in range(20):\n",
    "    optimizer.zero_grad()\n",
    "    output = (model(x))\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('Loss: {:.3f}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "straight-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 3\n",
    "# Create a custom loss function that allows for our original categorical labels\n",
    "# We could use weighted BCE as a starting point to weight incorrect positive predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
